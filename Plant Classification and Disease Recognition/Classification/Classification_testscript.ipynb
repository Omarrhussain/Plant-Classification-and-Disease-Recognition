{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2415961,"sourceType":"datasetVersion","datasetId":1365967},{"sourceId":10132148,"sourceType":"datasetVersion","datasetId":6253285},{"sourceId":10152535,"sourceType":"datasetVersion","datasetId":6267908},{"sourceId":10219924,"sourceType":"datasetVersion","datasetId":6317732},{"sourceId":10219926,"sourceType":"datasetVersion","datasetId":6317734},{"sourceId":10220470,"sourceType":"datasetVersion","datasetId":6318114},{"sourceId":10226254,"sourceType":"datasetVersion","datasetId":6322311},{"sourceId":10227256,"sourceType":"datasetVersion","datasetId":6321968},{"sourceId":10230297,"sourceType":"datasetVersion","datasetId":6325298},{"sourceId":10230319,"sourceType":"datasetVersion","datasetId":6325316},{"sourceId":10230719,"sourceType":"datasetVersion","datasetId":6325591},{"sourceId":10233494,"sourceType":"datasetVersion","datasetId":6327581},{"sourceId":10233682,"sourceType":"datasetVersion","datasetId":6327713},{"sourceId":201612,"sourceType":"modelInstanceVersion","modelInstanceId":172009,"modelId":194343},{"sourceId":201814,"sourceType":"modelInstanceVersion","modelInstanceId":172185,"modelId":194523}],"dockerImageVersionId":30177,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Face Recognition**\n\nFacial Recognition System is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image.\n\nWe'll be building a face recognition model that uses Siamese Networks to give us a distance value that indicates whether 2 images are same or different.\n\nThe Dataset\nWe'll be using the Extracted Faces from face-recognition-dataset, which is derived from the LFW Dataset. The Extracted Faces contains faces extracted from the base images using Haar-Cascade Face-Detection (CV2).\n\n* The dataset contains 1324 different individuals, with 2-50 images per person.\n* The images are of size (128,128,3) and are encoded in RGB.\n* Each folder and image is named with a number, i.e 0.jpg, 1.jpg","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport random\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ntf.__version__, np.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:45:59.723212Z","iopub.execute_input":"2024-12-18T08:45:59.723511Z","iopub.status.idle":"2024-12-18T08:46:06.182773Z","shell.execute_reply.started":"2024-12-18T08:45:59.723429Z","shell.execute_reply":"2024-12-18T08:46:06.182074Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"('2.6.2', '1.20.3')"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\nfrom tensorflow.keras.models import load_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:06.184125Z","iopub.execute_input":"2024-12-18T08:46:06.184658Z","iopub.status.idle":"2024-12-18T08:46:06.188981Z","shell.execute_reply.started":"2024-12-18T08:46:06.184621Z","shell.execute_reply":"2024-12-18T08:46:06.188295Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model = load_model('/kaggle/input/mobilenet-classification/MobileNet_saved_model.h5')\n# import tensorflow.keras.backend as K\n# def custom_function(x):\n#     return K.square(x) \nencoder=load_model('/kaggle/input/new_encoder/keras/default/1/last_encoder(90%_test).h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:06.189925Z","iopub.execute_input":"2024-12-18T08:46:06.190115Z","iopub.status.idle":"2024-12-18T08:46:18.317944Z","shell.execute_reply.started":"2024-12-18T08:46:06.190092Z","shell.execute_reply":"2024-12-18T08:46:18.317318Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport cv2\nimport numpy as np\n\ndef preprocess_new_image(image_path, target_size=(224, 224)):\n    \"\"\"\n    Preprocess the image to prepare it for the model.\n    Args:\n        image_path (str): Path to the new image.\n        target_size (tuple): Target size to resize the image.\n    Returns:\n        np.array: Preprocessed image.\n    \"\"\"\n    # Load and resize the image\n    image = load_img(image_path, target_size=target_size)\n    \n    # Convert the PIL image to a NumPy array\n    image_array = img_to_array(image)  # Shape: (224, 224, 3)\n    \n    # Apply sharpening filter using OpenCV\n    kernel = np.array([[0, -1, 0],\n                       [-1, 5, -1],\n                       [0, -1, 0]]) \n    sharpened_image = cv2.filter2D(image_array.astype(np.uint8), -1, kernel)\n    \n    # Normalize pixel values to [0, 1]\n    sharpened_image = sharpened_image / 255.0\n    \n    # Expand dimensions to match model input shape: (1, 224, 224, 3)\n    preprocessed_image = np.expand_dims(sharpened_image, axis=0)\n    \n    return preprocessed_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:18.320655Z","iopub.execute_input":"2024-12-18T08:46:18.320968Z","iopub.status.idle":"2024-12-18T08:46:18.328858Z","shell.execute_reply.started":"2024-12-18T08:46:18.320930Z","shell.execute_reply":"2024-12-18T08:46:18.328117Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\nimport os\n\ndef load_images(folder_path):\n    all_diseases = sorted(os.listdir(folder_path))\n    images = []\n    labels = []\n    for class_label in all_diseases:\n        class_path = os.path.join(folder_path, class_label)\n        if os.path.isdir(class_path):\n            for filename in os.listdir(class_path):\n                img_path = os.path.join(class_path, filename)\n\n                img_array = preprocess_new_image(img_path)  # Assume this function processes the image correctly\n                class_label = class_label.split(\"___\")[0]  # Assuming labels are embedded in folder names\n\n                images.append(img_array)\n                labels.append(class_label)\n    return np.vstack(images), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:18.329809Z","iopub.execute_input":"2024-12-18T08:46:18.330039Z","iopub.status.idle":"2024-12-18T08:46:18.496016Z","shell.execute_reply.started":"2024-12-18T08:46:18.330007Z","shell.execute_reply":"2024-12-18T08:46:18.495417Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load test data\nX_test, y_test = load_images('/kaggle/input/disscus/test data/Test Data/Test Data/Test samples for stage 1 (classification) and stage 2 (recognition)')\n\n# Predict using the model\ny_pred_label = model.predict(X_test)  # Assuming model is preloaded and trained","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:18.496906Z","iopub.execute_input":"2024-12-18T08:46:18.497099Z","iopub.status.idle":"2024-12-18T08:46:26.718220Z","shell.execute_reply.started":"2024-12-18T08:46:18.497073Z","shell.execute_reply":"2024-12-18T08:46:26.717376Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Map predicted numeric labels to class names if needed\n\nclass_mapping = {\n    0: \"Apple\",\n    1: \"Cherry_(including_sour)\",\n    2: \"Corn_(maize)\",\n    3: \"Grape\",\n    4: \"Peach\",\n    5: \"Pepper,_bell\",\n    6: \"Potato\",\n    7: \"Strawberry\",\n    8: \"Tomato\"\n}\nif hasattr(model, \"classes\"):  # Some sklearn models have classes_ attribute\n    ypred = [model.classes[np.argmax(pred)] for pred in y_pred_label]\nelse:\n    y_pred = [class_mapping.get(np.argmax(pred), \"Unknown\") for pred in y_pred_label]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:26.719583Z","iopub.execute_input":"2024-12-18T08:46:26.719879Z","iopub.status.idle":"2024-12-18T08:46:26.726587Z","shell.execute_reply.started":"2024-12-18T08:46:26.719843Z","shell.execute_reply":"2024-12-18T08:46:26.725909Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\nimport os\n\n\n\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n\n\n# Ensure y_test and y_pred are comparable numpy arrays\ny_pred = np.array(y_pred)  # Predictions\ny_test = np.array(y_test)  # Ground truth labels\n\n# Check shapes before accuracy calculation\nif y_pred.shape != y_test.shape:\n    raise ValueError(f\"Shape mismatch: y_test {y_test.shape}, y_pred {y_pred.shape}\")\n\n# Print predictions and corresponding ground truth labels\nprint(\"Predictions vs Ground Truth:\")\nfor i in range(len(y_test)):\n    print(f\"y_test: {y_test[i]} , y_pred: {y_pred[i]}\")\n\n# Calculate accuracy\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc * 100:.2f}%\")\n\n\n\n\n\n\n# def classify_images(face_list1, face_list2, threshold=1.2):\n#     \"\"\"\n#     Compares two images using the encoder model and returns predictions.\n#     \"\"\"\n#     # Get encodings for the passed images\n#     tensor1 = encoder.predict(face_list1)\n#     tensor2 = encoder.predict(face_list2)\n\n#     # Calculate the distance between encodings\n#     distance = np.sum(np.square(tensor1 - tensor2), axis=-1)\n\n#     # Prediction: 0 if similar, 1 if dissimilar\n#     prediction = np.where(distance <= threshold, 0, 1)\n#     return prediction# Ensure y_test and y_pred are comparable\n# y_pred = np.array(y_pred)\n# y_test = np.array(y_test)\n\n# # Calculate accuracy\n# # for i in range(len(y_test)):\n\n# #     print(f\"y_test : {y_test[i]} , y_pred : {y_pred[i]}\" )\n\n# # print(\"y_pred (label) :\" ,y_pred)\n# acc = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", acc * 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:26.727476Z","iopub.execute_input":"2024-12-18T08:46:26.727675Z","iopub.status.idle":"2024-12-18T08:46:26.849695Z","shell.execute_reply.started":"2024-12-18T08:46:26.727639Z","shell.execute_reply":"2024-12-18T08:46:26.848933Z"}},"outputs":[{"name":"stdout","text":"Predictions vs Ground Truth:\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Apple , y_pred: Apple\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Apple\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Cherry_(including_sour) , y_pred: Cherry_(including_sour)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Corn_(maize) , y_pred: Corn_(maize)\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Grape , y_pred: Grape\ny_test: Peach , y_pred: Apple\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Peach , y_pred: Peach\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Pepper,_bell , y_pred: Pepper,_bell\ny_test: Potato , y_pred: Potato\ny_test: Potato , y_pred: Potato\ny_test: Potato , y_pred: Apple\ny_test: Potato , y_pred: Potato\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Apple\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Strawberry , y_pred: Strawberry\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Apple\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Potato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\ny_test: Tomato , y_pred: Tomato\nAccuracy: 96.61%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\nimport cv2\nimport numpy as np\n\ndef preprocess_new_image(img_path):\n    # Read and resize the image\n    img = cv2.imread(img_path)\n    if img is None:\n        raise FileNotFoundError(f\"Image at {img_path} could not be loaded. Check the path.\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n    img_resized = cv2.resize(img, (128, 128))   # Resize to (128, 128)\n    img_normalized = img_resized / 255.0        # Normalize the image\n    \n    # Expand dimensions to add the batch size: (1, 128, 128, 3)\n    img_batch = np.expand_dims(img_normalized, axis=0)\n    return img_batch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:26.850801Z","iopub.execute_input":"2024-12-18T08:46:26.851045Z","iopub.status.idle":"2024-12-18T08:46:26.857630Z","shell.execute_reply.started":"2024-12-18T08:46:26.851009Z","shell.execute_reply":"2024-12-18T08:46:26.856825Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport numpy as np\n\ndef load_images_rec(folder_path):\n    all_diseases = sorted(os.listdir(folder_path))\n    images = []\n    labels = []\n    for class_label in all_diseases:\n        class_path = os.path.join(folder_path, class_label)\n        if os.path.isdir(class_path):  # Check if it's a directory\n            for filename in os.listdir(class_path):\n                img_path = os.path.join(class_path, filename)\n                try:\n                    # Process the image and append it to the list\n                    img_array = preprocess_new_image(img_path)\n                    images.append(img_array)\n                    labels.append(class_label)\n                except Exception as e:\n                    print(f\"Error processing {img_path}: {e}\")\n    \n    # Stack all the images and labels\n    return np.vstack(images), np.array(labels)\n\n\nX_test2, y_test2 = load_images_rec('/kaggle/input/disscus/test data/Test Data/Test Data/Test samples for stage 1 (classification) and stage 2 (recognition)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:26.859833Z","iopub.execute_input":"2024-12-18T08:46:26.860079Z","iopub.status.idle":"2024-12-18T08:46:27.310372Z","shell.execute_reply.started":"2024-12-18T08:46:26.860056Z","shell.execute_reply":"2024-12-18T08:46:27.309787Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"len(y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:27.311344Z","iopub.execute_input":"2024-12-18T08:46:27.311554Z","iopub.status.idle":"2024-12-18T08:46:27.316326Z","shell.execute_reply.started":"2024-12-18T08:46:27.311524Z","shell.execute_reply":"2024-12-18T08:46:27.315691Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"177"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\n\n\ndef classify_images(face_list1, face_list2):\n    \"\"\"\n    Compares two images using the encoder model and returns distances.\n    \"\"\"\n    # Ensure inputs have batch dimensions (expand if necessary)\n    if len(face_list1.shape) == 3:  # (128, 128, 3)\n        face_list1 = np.expand_dims(face_list1, axis=0)  # Add batch dimension\n    if len(face_list2.shape) == 3:\n        face_list2 = np.expand_dims(face_list2, axis=0)\n\n    # Get encodings for the passed images\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n\n    # Calculate the distance between encodings\n    distance = np.sum(np.square(tensor1 - tensor2), axis=-1)\n    \n    return distance\n\n\ndef xyz(target_batch, label):\n    \"\"\"\n    Finds the subfolder with the minimum distance to the target image.\n    \"\"\"\n    comparison_results = {}\n    folder_path = f'/kaggle/input/support/Support_set/one shot/{label}'\n\n    # Iterate over each subfolder\n    for subfolder in os.listdir(folder_path):\n        subfolder_path = os.path.join(folder_path, subfolder)\n        if os.path.isdir(subfolder_path):  # Ensure it's a directory\n            comparison_results[subfolder] = []\n\n            # Iterate over all image files in the subfolder\n            for file_name in os.listdir(subfolder_path):\n                file_path = os.path.join(subfolder_path, file_name)\n\n                # Check if it's an image file\n                if file_name.endswith(('.JPG', '.JPEG', '.PNG')):\n                    image = cv2.imread(file_path)\n                    if image is not None:\n                        image_resized = cv2.resize(image, (128, 128))\n                        image_normalized = image_resized / 255.0\n                        image_batch = np.expand_dims(image_normalized, axis=0)\n\n                        # Compare target image with current image\n                        distance = classify_images(target_batch, image_batch)\n                        # Append the distance to the comparison results\n                        comparison_results[subfolder].append(distance[0])\n                        # print(comparison_results)\n\n    # Find the subfolder with the minimum distance\n    min_distance = float('inf')\n    best_subfolder = None\n\n    for subfolder, distances in comparison_results.items():\n        if distances:\n            # Get the minimum distance in the current subfolder\n            subfolder_min_distance = min(distances)\n\n            # Update the global minimum if this subfolder has a smaller distance\n            if subfolder_min_distance < min_distance:\n                min_distance = subfolder_min_distance\n                best_subfolder = subfolder\n\n    if best_subfolder is not None:\n        print(f\"Subfolder with Minimum Distance: {best_subfolder} , Minimum Distance: {min_distance:.4f}\")\n        return best_subfolder\n    else:\n        print(\"No subfolders with images to compare.\")\n        return None\n\n\n# Assuming X_test2 and y_pred are defined and populated correctly before this\nReco = []\ny_pred2 = []\n\n# Populate Reco with tuples of (image_batch, label) for each sample\nfor i in range(len(y_pred)):\n    Reco.append((X_test2[i], y_pred[i]))\n\n# Iterate over Reco to call the xyz function and append the result to y_pred2\nfor i in range(len(y_pred)):\n    Image, label = Reco[i]\n    Back = xyz(Image, label)\n    y_pred2.append(Back)\n\n# Now, y_pred2 contains the subfolders with the smallest distance for each comparison\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:46:27.317276Z","iopub.execute_input":"2024-12-18T08:46:27.317458Z","iopub.status.idle":"2024-12-18T08:48:15.743012Z","shell.execute_reply.started":"2024-12-18T08:46:27.317436Z","shell.execute_reply":"2024-12-18T08:48:15.742153Z"}},"outputs":[{"name":"stdout","text":"Subfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.5275\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.4219\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.1491\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.7876\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.9010\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.3292\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.1694\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.0495\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.0704\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 0.1454\nSubfolder with Minimum Distance: Apple___healthy , Minimum Distance: 0.6782\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.5468\nSubfolder with Minimum Distance: Apple___healthy , Minimum Distance: 0.7701\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.9169\nSubfolder with Minimum Distance: Apple___healthy , Minimum Distance: 0.2354\nSubfolder with Minimum Distance: Apple___healthy , Minimum Distance: 1.0706\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.0738\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.0310\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.2967\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.0685\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.4690\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.1364\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.2446\nSubfolder with Minimum Distance: Cherry_(including_sour)___Powdery_mildew , Minimum Distance: 0.0423\nSubfolder with Minimum Distance: Cherry_(including_sour)___healthy , Minimum Distance: 0.0930\nSubfolder with Minimum Distance: Apple___healthy , Minimum Distance: 1.2866\nSubfolder with Minimum Distance: Cherry_(including_sour)___healthy , Minimum Distance: 0.2173\nSubfolder with Minimum Distance: Cherry_(including_sour)___healthy , Minimum Distance: 0.0955\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0645\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.6288\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0469\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.2720\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0620\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.6118\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0426\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0656\nSubfolder with Minimum Distance: Corn_(maize)___Common_rust_ , Minimum Distance: 0.0536\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.5099\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.3709\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.2449\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.3256\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.5239\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.4908\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.3985\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.3003\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.2380\nSubfolder with Minimum Distance: Corn_(maize)___Northern_Leaf_Blight , Minimum Distance: 0.2640\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3513\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3630\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3337\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3458\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3277\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3473\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3293\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3194\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3281\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3232\nSubfolder with Minimum Distance: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot , Minimum Distance: 0.3426\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.1232\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.5041\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.5261\nSubfolder with Minimum Distance: Grape___healthy , Minimum Distance: 0.2909\nSubfolder with Minimum Distance: Grape___Black_rot , Minimum Distance: 0.4042\nSubfolder with Minimum Distance: Grape___healthy , Minimum Distance: 0.3475\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.4700\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.3178\nSubfolder with Minimum Distance: Grape___healthy , Minimum Distance: 0.3372\nSubfolder with Minimum Distance: Grape___Black_rot , Minimum Distance: 0.4537\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.4363\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.4896\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.4320\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.2787\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.2507\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.0891\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.1444\nSubfolder with Minimum Distance: Grape___healthy , Minimum Distance: 0.2905\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.1512\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.1130\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.3660\nSubfolder with Minimum Distance: Grape___Esca_(Black_Measles) , Minimum Distance: 0.3620\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.2408\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.0614\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.1193\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.1074\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.7440\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.6828\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.7687\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.0634\nSubfolder with Minimum Distance: Grape___Leaf_blight_(Isariopsis_Leaf_Spot) , Minimum Distance: 0.1442\nSubfolder with Minimum Distance: Grape___healthy , Minimum Distance: 0.0591\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 0.9075\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.3717\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 0.9717\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 1.7501\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 0.4881\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 1.5481\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 1.2714\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.6046\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 0.9930\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1583\nSubfolder with Minimum Distance: Peach___Bacterial_spot , Minimum Distance: 0.1978\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.3883\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.5114\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1438\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1310\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1423\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1705\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.0708\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.3272\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.1094\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.0612\nSubfolder with Minimum Distance: Peach___healthy , Minimum Distance: 0.2923\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 1.0595\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.3072\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.1964\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.3078\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.5757\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.9501\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 1.0142\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.7847\nSubfolder with Minimum Distance: Pepper,_bell___healthy , Minimum Distance: 0.2656\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.3022\nSubfolder with Minimum Distance: Pepper,_bell___Bacterial_spot , Minimum Distance: 0.5994\nSubfolder with Minimum Distance: Pepper,_bell___healthy , Minimum Distance: 0.1527\nSubfolder with Minimum Distance: Pepper,_bell___healthy , Minimum Distance: 0.2926\nSubfolder with Minimum Distance: Potato___Late_blight , Minimum Distance: 0.4001\nSubfolder with Minimum Distance: Potato___healthy , Minimum Distance: 0.0770\nSubfolder with Minimum Distance: Apple___Apple_scab , Minimum Distance: 1.6449\nSubfolder with Minimum Distance: Potato___healthy , Minimum Distance: 0.0728\nSubfolder with Minimum Distance: Strawberry___Leaf_scorch , Minimum Distance: 0.7100\nSubfolder with Minimum Distance: Strawberry___Leaf_scorch , Minimum Distance: 1.6105\nSubfolder with Minimum Distance: Strawberry___Leaf_scorch , Minimum Distance: 1.1251\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.6989\nSubfolder with Minimum Distance: Apple___Black_rot , Minimum Distance: 1.3000\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.6342\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.5063\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.4500\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.7088\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.3771\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 0.9646\nSubfolder with Minimum Distance: Strawberry___healthy , Minimum Distance: 1.2321\nSubfolder with Minimum Distance: Tomato___Early_blight , Minimum Distance: 0.1509\nSubfolder with Minimum Distance: Apple___Cedar_apple_rust , Minimum Distance: 1.9505\nSubfolder with Minimum Distance: Tomato___Leaf_Mold , Minimum Distance: 0.5419\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.6295\nSubfolder with Minimum Distance: Tomato___Septoria_leaf_spot , Minimum Distance: 0.9256\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.6572\nSubfolder with Minimum Distance: Tomato___Early_blight , Minimum Distance: 1.2267\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 1.1307\nSubfolder with Minimum Distance: Tomato___Leaf_Mold , Minimum Distance: 0.2593\nSubfolder with Minimum Distance: Tomato___Tomato_mosaic_virus , Minimum Distance: 0.9044\nSubfolder with Minimum Distance: Tomato___Tomato_mosaic_virus , Minimum Distance: 1.1630\nSubfolder with Minimum Distance: Tomato___Early_blight , Minimum Distance: 0.8924\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.1973\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.5647\nSubfolder with Minimum Distance: Tomato___Tomato_mosaic_virus , Minimum Distance: 0.8162\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.5771\nSubfolder with Minimum Distance: Tomato___Tomato_Yellow_Leaf_Curl_Virus , Minimum Distance: 0.2805\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.2818\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 1.2748\nSubfolder with Minimum Distance: Potato___Late_blight , Minimum Distance: 0.3659\nSubfolder with Minimum Distance: Tomato___Septoria_leaf_spot , Minimum Distance: 0.9536\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.3147\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.1451\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.5076\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.3034\nSubfolder with Minimum Distance: Tomato___Spider_mites Two-spotted_spider_mite , Minimum Distance: 0.1828\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.9240\nSubfolder with Minimum Distance: Tomato___Tomato_mosaic_virus , Minimum Distance: 0.3691\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.6951\nSubfolder with Minimum Distance: Tomato___Early_blight , Minimum Distance: 0.3256\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.0228\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.7067\nSubfolder with Minimum Distance: Tomato___healthy , Minimum Distance: 0.7021\nSubfolder with Minimum Distance: Tomato___Tomato_Yellow_Leaf_Curl_Virus , Minimum Distance: 0.2374\nSubfolder with Minimum Distance: Tomato___Bacterial_spot , Minimum Distance: 0.4412\nSubfolder with Minimum Distance: Tomato___Tomato_Yellow_Leaf_Curl_Virus , Minimum Distance: 0.5635\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Reco=[]\n# # Reco.append((X_test2,y_pred))\n# y_pred2=[]\n# for  i in range(len(y_pred)):\n#     Reco.append( (X_test2[i] , y_pred[i]) )\n\n# print()\n\n# for i in range (len(y_pred)):\n#     Image,label=Reco[i]\n#     # print(label,'\\n')\n#     Back=xyz(Image,label)\n#     y_pred2.append(Back)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:15.743938Z","iopub.execute_input":"2024-12-18T08:48:15.744108Z","iopub.status.idle":"2024-12-18T08:48:15.747734Z","shell.execute_reply.started":"2024-12-18T08:48:15.744087Z","shell.execute_reply":"2024-12-18T08:48:15.746984Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test2 = np.array(y_test2)\nacc = accuracy_score(y_test2, y_pred2)\nprint(\"Accuracy:\", acc * 100)\ny_pred2 = np.array(y_pred2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:15.748659Z","iopub.execute_input":"2024-12-18T08:48:15.748916Z","iopub.status.idle":"2024-12-18T08:48:15.760065Z","shell.execute_reply.started":"2024-12-18T08:48:15.748881Z","shell.execute_reply":"2024-12-18T08:48:15.759476Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 59.32203389830508\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def classify_images(face_list1, face_list2, threshold=1.2):\n    \"\"\"\n    Compares two images using the encoder model and returns predictions.\n    \"\"\"\n    # Ensure inputs have batch dimensions (expand if necessary)\n    if len(face_list1.shape) == 3:  # (128, 128, 3)\n        face_list1 = np.expand_dims(face_list1, axis=0)  # Add batch dimension\n    if len(face_list2.shape) == 3:\n        face_list2 = np.expand_dims(face_list2, axis=0)\n\n    # Get encodings for the passed images\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n\n    # Calculate the distance between encodings\n    distance = np.sum(np.square(tensor1 - tensor2), axis=-1)\n\n    # Prediction: 0 if similar, 1 if dissimilar\n    prediction = np.where(distance <= threshold, 0, 1)\n    return prediction\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:15.761121Z","iopub.execute_input":"2024-12-18T08:48:15.761633Z","iopub.status.idle":"2024-12-18T08:48:15.774500Z","shell.execute_reply.started":"2024-12-18T08:48:15.761597Z","shell.execute_reply":"2024-12-18T08:48:15.773807Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Function to preprocess the image\n\n\ndef classify_images(face_list1, face_list2):\n    \"\"\"\n    Compares two images using the encoder model and returns distances.\n    \"\"\"\n    # Ensure inputs have batch dimensions (expand if necessary)\n    if len(face_list1.shape) == 3:  # (128, 128, 3)\n        face_list1 = np.expand_dims(face_list1, axis=0)  # Add batch dimension\n    if len(face_list2.shape) == 3:\n        face_list2 = np.expand_dims(face_list2, axis=0)\n\n    # Get encodings for the passed images\n    tensor1 = encoder.predict(face_list1)\n    tensor2 = encoder.predict(face_list2)\n\n    # Calculate the distance between encodings\n    distance = np.sum(np.square(tensor1 - tensor2), axis=-1)\n    \n    return distance\n\n\ndef preprocess_image(image, target_size=(128, 128)):\n    \"\"\"\n    Resize and normalize the image for model input.\n    \"\"\"\n    image = cv2.resize(image, target_size)  # Resize image\n    image = image / 255.0  # Normalize pixel values\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    return image\n\n# Function to classify images\n\n\n# Function to compare anchor image with all other images\ndef compare_anchor_with_images(folder_path):\n    \"\"\"\n    Compare the anchor image with all other images in the folder.\n    \"\"\"\n    anchor_image = None\n    comparison_results = []\n\n    # Step 1: Locate the anchor image\n    for file_name in os.listdir(folder_path):\n        if 'Anchor' in file_name:  # Assumes anchor has \"anchor\" in its name\n            file_path = os.path.join(folder_path, file_name)\n            anchor_image = cv2.imread(file_path)\n            if anchor_image is not None:\n                print(f\"Anchor image found: {file_name}\")\n                anchor_image = preprocess_image(anchor_image)  # Preprocess it\n                break\n\n    # Ensure anchor image was found\n    if anchor_image is None:\n        print(\"Error: No anchor image found!\")\n        return\n\n    # Step 2: Compare the anchor image with other images\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        \n        # Skip the anchor image\n        if 'Anchor' in file_name:\n            continue\n\n        # Process and compare each image\n        if file_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n            current_image = cv2.imread(file_path)\n            if current_image is not None:\n                current_image = preprocess_image(current_image)\n                prediction = classify_images(anchor_image, current_image)\n                comparison_results.append((file_name, prediction))\n                \n    # Print the results\n    print(\"Comparison Results:\")\n    for file_name, result in comparison_results:\n        status = \"Similar\" if result <= 0.09 else \"Dissimilar\"\n        print(f\"{file_name}: {status}\")\n    \n        \n\n# Folder path where images are stored\nfolder_path = \"/kaggle/input/disscus/test data/Test Data/Test Data/One-few shot recognition part 2\"\ncompare_anchor_with_images(folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:15.775321Z","iopub.execute_input":"2024-12-18T08:48:15.775490Z","iopub.status.idle":"2024-12-18T08:48:17.494855Z","shell.execute_reply.started":"2024-12-18T08:48:15.775467Z","shell.execute_reply":"2024-12-18T08:48:17.494143Z"}},"outputs":[{"name":"stdout","text":"Anchor image found: Anchor.JPG\nComparison Results:\nPotato_healthy.JPG: Dissimilar\nSquash_Powdery_Mildew.JPG: Dissimilar\nPotato_late_blight.JPG: Dissimilar\nBlueberry_healthy.JPG: Dissimilar\nRasperry_healthy.JPG: Dissimilar\nSoybean_healthy.JPG: Dissimilar\nApple_scap.JPG: Dissimilar\nBlueberry_healthy2.JPG: Dissimilar\nTomato_target_spot.JPG: Dissimilar\nOrange_Citrus_greening.JPG: Similar\nPeach_Bacterial_spot.JPG: Dissimilar\nHibisicus_plant_gudhal.png: Dissimilar\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading the Dataset\nWe're reading the folders and splitting them into train and test set for training purposes.","metadata":{}},{"cell_type":"code","source":"# # Setting random seeds to enable consistency while testing.\n# random.seed(5)\n# np.random.seed(5)\n# tf.random.set_seed(5)\n\n# ROOT = \"../input/face-recognition-dataset/Extracted Faces/Extracted Faces\"\n\n# def read_image(index):\n#     path = os.path.join(ROOT, index[0], index[1])\n#     image = cv2.imread(path)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#     return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.495814Z","iopub.execute_input":"2024-12-18T08:48:17.496002Z","iopub.status.idle":"2024-12-18T08:48:17.500373Z","shell.execute_reply.started":"2024-12-18T08:48:17.495979Z","shell.execute_reply":"2024-12-18T08:48:17.499592Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# def split_dataset(directory, split=0.9):\n#     folders = os.listdir(directory)\n#     num_train = int(len(folders)*split)\n    \n#     random.shuffle(folders)\n    \n#     train_list, test_list = {}, {}\n    \n#     # Creating Train-list\n#     for folder in folders[:num_train]:\n#         num_files = len(os.listdir(os.path.join(directory, folder)))\n#         train_list[folder] = num_files\n    \n#     # Creating Test-list\n#     for folder in folders[num_train:]:\n#         num_files = len(os.listdir(os.path.join(directory, folder)))\n#         test_list[folder] = num_files  \n    \n#     return train_list, test_list\n\n# train_list, test_list = split_dataset(ROOT, split=0.9)\n# print(\"Length of training list:\", len(train_list))\n# print(\"Length of testing list :\", len(test_list))\n\n# # train_list, test list contains the folder names along with the number of files in the folder.\n# print(\"\\nTest List:\", test_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.501594Z","iopub.execute_input":"2024-12-18T08:48:17.501890Z","iopub.status.idle":"2024-12-18T08:48:17.514931Z","shell.execute_reply.started":"2024-12-18T08:48:17.501853Z","shell.execute_reply":"2024-12-18T08:48:17.514194Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Creating Triplets\nWe use the train and test list to create triplets of (anchor, postive, negative) face data, where positive is the same person and negative is a different person than anchor.","metadata":{}},{"cell_type":"code","source":"# def create_triplets(directory, folder_list, max_files=10):\n#     triplets = []\n#     folders = list(folder_list.keys())\n    \n#     for folder in folders:\n#         path = os.path.join(directory, folder)\n#         files = list(os.listdir(path))[:max_files]\n#         num_files = len(files)\n        \n#         for i in range(num_files-1):\n#             for j in range(i+1, num_files):\n#                 anchor = (folder, f\"{i}.jpg\")\n#                 positive = (folder, f\"{j}.jpg\")\n\n#                 neg_folder = folder\n#                 while neg_folder == folder:\n#                     neg_folder = random.choice(folders)\n#                 neg_file = random.randint(0, folder_list[neg_folder]-1)\n#                 negative = (neg_folder, f\"{neg_file}.jpg\")\n\n#                 triplets.append((anchor, positive, negative))\n            \n#     random.shuffle(triplets)\n#     return triplets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.515938Z","iopub.execute_input":"2024-12-18T08:48:17.516172Z","iopub.status.idle":"2024-12-18T08:48:17.526336Z","shell.execute_reply.started":"2024-12-18T08:48:17.516140Z","shell.execute_reply":"2024-12-18T08:48:17.525637Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# train_triplet = create_triplets(ROOT, train_list)\n# test_triplet  = create_triplets(ROOT, test_list)\n\n# print(\"Number of training triplets:\", len(train_triplet))\n# print(\"Number of testing triplets :\", len(test_triplet))\n\n# print(\"\\nExamples of triplets:\")\n# for i in range(5):\n#     print(train_triplet[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.527335Z","iopub.execute_input":"2024-12-18T08:48:17.527584Z","iopub.status.idle":"2024-12-18T08:48:17.540299Z","shell.execute_reply.started":"2024-12-18T08:48:17.527547Z","shell.execute_reply":"2024-12-18T08:48:17.539632Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Creating Batch-Generator\nCreating a Batch-Generator that converts the triplets passed into batches of face-data and preproccesses it before returning the data into seperate lists.\n\nParameters:\n\n* Batch_size: Batch_size of the data to return\n* Preprocess: Whether to preprocess the data or not","metadata":{}},{"cell_type":"code","source":"# def get_batch(triplet_list, batch_size=256, preprocess=True):\n#     batch_steps = len(triplet_list)//batch_size\n    \n#     for i in range(batch_steps+1):\n#         anchor   = []\n#         positive = []\n#         negative = []\n        \n#         j = i*batch_size\n#         while j<(i+1)*batch_size and j<len(triplet_list):\n#             a, p, n = triplet_list[j]\n#             anchor.append(read_image(a))\n#             positive.append(read_image(p))\n#             negative.append(read_image(n))\n#             j+=1\n            \n#         anchor = np.array(anchor)\n#         positive = np.array(positive)\n#         negative = np.array(negative)\n        \n#         if preprocess:\n#             anchor = preprocess_input(anchor)\n#             positive = preprocess_input(positive)\n#             negative = preprocess_input(negative)\n        \n#         yield ([anchor, positive, negative])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.541298Z","iopub.execute_input":"2024-12-18T08:48:17.541485Z","iopub.status.idle":"2024-12-18T08:48:17.550326Z","shell.execute_reply.started":"2024-12-18T08:48:17.541463Z","shell.execute_reply":"2024-12-18T08:48:17.549705Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Plotting the Data\nPlotting the data generated from get_batch() to see the results","metadata":{}},{"cell_type":"code","source":"# num_plots = 6\n\n# f, axes = plt.subplots(num_plots, 3, figsize=(15, 20))\n\n# for x in get_batch(train_triplet, batch_size=num_plots, preprocess=False):\n#     a,p,n = x\n#     for i in range(num_plots):\n#         axes[i, 0].imshow(a[i])\n#         axes[i, 1].imshow(p[i])\n#         axes[i, 2].imshow(n[i])\n#         i+=1\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.551326Z","iopub.execute_input":"2024-12-18T08:48:17.551507Z","iopub.status.idle":"2024-12-18T08:48:17.562013Z","shell.execute_reply.started":"2024-12-18T08:48:17.551486Z","shell.execute_reply":"2024-12-18T08:48:17.561316Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Creating the Model\nUnlike a conventional CNN, the Siamese Network does not classify the images into certain categories or labels, rather it only finds out the distance between any two given images. If the images have the same label, then the network should learn the parameters, i.e. the weights and the biases in such a way that it should produce a smaller distance between the two images, and if they belong to different labels, then the distance should be larger\n\nSiamese Network Image\n![](https://miro.medium.com/max/2000/1*05hUCDHhnl4hdjqvdVTHtw.png)","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras import backend, layers, metrics\n\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.applications import Xception\n# from tensorflow.keras.models import Model, Sequential\n\n# from tensorflow.keras.utils import plot_model\n# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.562812Z","iopub.execute_input":"2024-12-18T08:48:17.562967Z","iopub.status.idle":"2024-12-18T08:48:17.572540Z","shell.execute_reply.started":"2024-12-18T08:48:17.562948Z","shell.execute_reply":"2024-12-18T08:48:17.571906Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Encoder\nThe Encoder is responsible for converting the passed images into their feature vectors. We're using a pretrained model, Xception model which is based on Inception_V3 model. By using transfer learning, we can significantly reduce the training time and size of the dataset.\n\nThe Model is connected to Fully Connected (Dense) layers and the last layer normalises the data using L2 Normalisation. (L2 Normalisation is a technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1)","metadata":{}},{"cell_type":"code","source":"# def get_encoder(input_shape):\n#     \"\"\" Returns the image encoding model \"\"\"\n\n#     pretrained_model = Xception(\n#         input_shape=input_shape,\n#         weights='imagenet',\n#         include_top=False,\n#         pooling='avg',\n#     )\n    \n#     for i in range(len(pretrained_model.layers)-27):\n#         pretrained_model.layers[i].trainable = False\n\n#     encode_model = Sequential([\n#         pretrained_model,\n#         layers.Flatten(),\n#         layers.Dense(512, activation='relu'),\n#         layers.BatchNormalization(),\n#         layers.Dense(256, activation=\"relu\"),\n#         layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n#     ], name=\"Encode_Model\")\n#     return encode_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.573477Z","iopub.execute_input":"2024-12-18T08:48:17.573761Z","iopub.status.idle":"2024-12-18T08:48:17.581976Z","shell.execute_reply.started":"2024-12-18T08:48:17.573728Z","shell.execute_reply":"2024-12-18T08:48:17.581356Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"Siamese Network\nWe're creating a Siamese Network that takes 3 input images, (anchor, postive, negative) and uses the encoder above to encode the images to their feature vectors. Those features are passed to a distance layer which computes the distance between (anchor, positive) and (anchor, negative) pairs.\n\nWe'll be defining a custom layer to compute the distance.\n\nDistance Formula:\n\n![](data:image/png;base64,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)","metadata":{}},{"cell_type":"code","source":"# class DistanceLayer(layers.Layer):\n#     # A layer to compute f(A) - f(P) and f(A) - f(N)\n#     def __init__(self, **kwargs):\n#         super().__init__(**kwargs)\n\n#     def call(self, anchor, positive, negative):\n#         ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n#         an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n#         return (ap_distance, an_distance)\n    \n\n# def get_siamese_network(input_shape = (128, 128, 3)):\n#     encoder = get_encoder(input_shape)\n    \n#     # Input Layers for the images\n#     anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n#     positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n#     negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n    \n#     ## Generate the encodings (feature vectors) for the images\n#     encoded_a = encoder(anchor_input)\n#     encoded_p = encoder(positive_input)\n#     encoded_n = encoder(negative_input)\n    \n#     # A layer to compute f(A) - f(P) and f(A) - f(N)\n#     distances = DistanceLayer()(\n#         encoder(anchor_input),\n#         encoder(positive_input),\n#         encoder(negative_input)\n#     )\n    \n#     # Creating the Model\n#     siamese_network = Model(\n#         inputs  = [anchor_input, positive_input, negative_input],\n#         outputs = distances,\n#         name = \"Siamese_Network\"\n#     )\n#     return siamese_network\n\n# siamese_network = get_siamese_network()\n# siamese_network.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.582880Z","iopub.execute_input":"2024-12-18T08:48:17.583066Z","iopub.status.idle":"2024-12-18T08:48:17.596698Z","shell.execute_reply.started":"2024-12-18T08:48:17.583041Z","shell.execute_reply":"2024-12-18T08:48:17.596067Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# plot_model(siamese_network, show_shapes=True, show_layer_names=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.597601Z","iopub.execute_input":"2024-12-18T08:48:17.597859Z","iopub.status.idle":"2024-12-18T08:48:17.610196Z","shell.execute_reply.started":"2024-12-18T08:48:17.597825Z","shell.execute_reply":"2024-12-18T08:48:17.609465Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Putting everything together\nWe now need to implement a model with custom training loop and loss function so we can compute the triplet loss using the three embeddings produced by the Siamese network.\n\nWe'll create a Mean metric instance to track the loss of the training process.\n\nTriplet Loss Function:\n\n### ![](https://miro.medium.com/max/1838/0*AX2TSZNk19_gDgTN.png)","metadata":{}},{"cell_type":"code","source":"# class SiameseModel(Model):\n#     # Builds a Siamese model based on a base-model\n#     def __init__(self, siamese_network, margin=1.0):\n#         super(SiameseModel, self).__init__()\n        \n#         self.margin = margin\n#         self.siamese_network = siamese_network\n#         self.loss_tracker = metrics.Mean(name=\"loss\")\n\n#     def call(self, inputs):\n#         return self.siamese_network(inputs)\n\n#     def train_step(self, data):\n#         # GradientTape get the gradients when we compute loss, and uses them to update the weights\n#         with tf.GradientTape() as tape:\n#             loss = self._compute_loss(data)\n            \n#         gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n#         self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n        \n#         self.loss_tracker.update_state(loss)\n#         return {\"loss\": self.loss_tracker.result()}\n\n#     def test_step(self, data):\n#         loss = self._compute_loss(data)\n        \n#         self.loss_tracker.update_state(loss)\n#         return {\"loss\": self.loss_tracker.result()}\n\n#     def _compute_loss(self, data):\n#         # Get the two distances from the network, then compute the triplet loss\n#         ap_distance, an_distance = self.siamese_network(data)\n#         loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n#         return loss\n\n#     @property\n#     def metrics(self):\n#         # We need to list our metrics so the reset_states() can be called automatically.\n#         return [self.loss_tracker]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.611043Z","iopub.execute_input":"2024-12-18T08:48:17.611226Z","iopub.status.idle":"2024-12-18T08:48:17.620756Z","shell.execute_reply.started":"2024-12-18T08:48:17.611191Z","shell.execute_reply":"2024-12-18T08:48:17.619947Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# siamese_model = SiameseModel(siamese_network)\n\n# optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n# siamese_model.compile(optimizer=optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.623419Z","iopub.execute_input":"2024-12-18T08:48:17.623607Z","iopub.status.idle":"2024-12-18T08:48:17.633513Z","shell.execute_reply.started":"2024-12-18T08:48:17.623586Z","shell.execute_reply":"2024-12-18T08:48:17.632786Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## Training the Model\nWe'll now be training the siamese_model on batches of triplets. We'll print the training loss, along with additional metrics from testing every epoch. The model weights will also be saved whenever it outperforms the previous max_accuracy.\n\nWe're hoping to collect more metrics about the model to evaluate how to increase the accuracy of the model. The epochs have been set to avoid going over Kaggle's time constraint.\n\n### Test Function\ntest_on_triplets() function will be responsible for testing the model on test_triplets. It'll collect metrics (accuracy, means, stds) by predicting on the train data. We'll also be printing the Accuracy of the model after testing.","metadata":{}},{"cell_type":"code","source":"# def test_on_triplets(batch_size = 256):\n#     pos_scores, neg_scores = [], []\n\n#     for data in get_batch(test_triplet, batch_size=batch_size):\n#         prediction = siamese_model.predict(data)\n#         pos_scores += list(prediction[0])\n#         neg_scores += list(prediction[1])\n    \n#     accuracy = np.sum(np.array(pos_scores) < np.array(neg_scores)) / len(pos_scores)\n#     ap_mean = np.mean(pos_scores)\n#     an_mean = np.mean(neg_scores)\n#     ap_stds = np.std(pos_scores)\n#     an_stds = np.std(neg_scores)\n    \n#     print(f\"Accuracy on test = {accuracy:.5f}\")\n#     return (accuracy, ap_mean, an_mean, ap_stds, an_stds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.634580Z","iopub.execute_input":"2024-12-18T08:48:17.634923Z","iopub.status.idle":"2024-12-18T08:48:17.643504Z","shell.execute_reply.started":"2024-12-18T08:48:17.634885Z","shell.execute_reply":"2024-12-18T08:48:17.642895Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# save_all = False\n# epochs = 30\n# batch_size = 128\n\n# max_acc = 0\n# train_loss = []\n# test_metrics = []\n\n# for epoch in range(1, epochs+1):\n#     t = time.time()\n    \n#     # Training the model on train data\n#     epoch_loss = []\n#     for data in get_batch(train_triplet, batch_size=batch_size):\n#         loss = siamese_model.train_on_batch(data)\n#         epoch_loss.append(loss)\n#     epoch_loss = sum(epoch_loss)/len(epoch_loss)\n#     train_loss.append(epoch_loss)\n\n#     print(f\"\\nEPOCH: {epoch} \\t (Epoch done in {int(time.time()-t)} sec)\")\n#     print(f\"Loss on train    = {epoch_loss:.5f}\")\n    \n#     # Testing the model on test data\n#     metric = test_on_triplets(batch_size=batch_size)\n#     test_metrics.append(metric)\n#     accuracy = metric[0]\n    \n#     # Saving the model weights\n#     if save_all or accuracy>=max_acc:\n#         siamese_model.save_weights(\"siamese_model\")\n#         max_acc = accuracy\n\n# # Saving the model after all epochs run\n# siamese_model.save_weights(\"siamese_model-final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.644316Z","iopub.execute_input":"2024-12-18T08:48:17.644480Z","iopub.status.idle":"2024-12-18T08:48:17.658787Z","shell.execute_reply.started":"2024-12-18T08:48:17.644461Z","shell.execute_reply":"2024-12-18T08:48:17.658185Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Using the Model\nNow that we've finished training our model, we need to extract the encoder so that we can use it to encode images and then get use the feature vectors to compute the distance between those images.\n\nWe'll also be saving the encoder for later use.","metadata":{}},{"cell_type":"code","source":"# def extract_encoder(model):\n#     encoder = get_encoder((128, 128, 3))\n#     i=0\n#     for e_layer in model.layers[0].layers[3].layers:\n#         layer_weight = e_layer.get_weights()\n#         encoder.layers[i].set_weights(layer_weight)\n#         i+=1\n#     return encoder\n\n# encoder = extract_encoder(siamese_model)\n# encoder.save_weights(\"encoder\")\n# encoder.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.659572Z","iopub.execute_input":"2024-12-18T08:48:17.659760Z","iopub.status.idle":"2024-12-18T08:48:17.669811Z","shell.execute_reply.started":"2024-12-18T08:48:17.659738Z","shell.execute_reply":"2024-12-18T08:48:17.669206Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Classify Images\nTo compute the distance between the encodings of the images, we'll be using distance formula. Distance over a certain threshold to be \"different\" and below the threshold as \"same\".","metadata":{}},{"cell_type":"code","source":"# def classify_images(face_list1, face_list2, threshold=1.3):\n#     # Getting the encodings for the passed faces\n#     tensor1 = encoder.predict(face_list1)\n#     tensor2 = encoder.predict(face_list2)\n    \n#     distance = np.sum(np.square(tensor1-tensor2), axis=-1)\n#     prediction = np.where(distance<=threshold, 0, 1)\n#     return prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.670576Z","iopub.execute_input":"2024-12-18T08:48:17.670774Z","iopub.status.idle":"2024-12-18T08:48:17.680009Z","shell.execute_reply.started":"2024-12-18T08:48:17.670749Z","shell.execute_reply":"2024-12-18T08:48:17.679285Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# def ModelMetrics(pos_list, neg_list):\n#     true = np.array([0]*len(pos_list)+[1]*len(neg_list))\n#     pred = np.append(pos_list, neg_list)\n    \n#     # Compute and print the accuracy\n#     print(f\"\\nAccuracy of model: {accuracy_score(true, pred)}\\n\")\n    \n#     # Compute and plot the Confusion matrix\n#     cf_matrix = confusion_matrix(true, pred)\n\n#     categories  = ['Similar','Different']\n#     names = ['True Similar','False Similar', 'False Different','True Different']\n#     percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n#     labels = [f'{v1}\\n{v2}' for v1, v2 in zip(names, percentages)]\n#     labels = np.asarray(labels).reshape(2,2)\n\n#     sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n#                 xticklabels = categories, yticklabels = categories)\n\n#     plt.xlabel(\"Predicted\", fontdict = {'size':14}, labelpad = 10)\n#     plt.ylabel(\"Actual\"   , fontdict = {'size':14}, labelpad = 10)\n#     plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n\n\n# pos_list = np.array([])\n# neg_list = np.array([])\n\n# for data in get_batch(test_triplet, batch_size=256):\n#     a, p, n = data\n#     pos_list = np.append(pos_list, classify_images(a, p))\n#     neg_list = np.append(neg_list, classify_images(a, n))\n#     break\n\n# ModelMetrics(pos_list, neg_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:48:17.680961Z","iopub.execute_input":"2024-12-18T08:48:17.681163Z","iopub.status.idle":"2024-12-18T08:48:17.696312Z","shell.execute_reply.started":"2024-12-18T08:48:17.681135Z","shell.execute_reply":"2024-12-18T08:48:17.695562Z"}},"outputs":[],"execution_count":33}]}